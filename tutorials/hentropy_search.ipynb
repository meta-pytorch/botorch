{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H-Entropy Search\n",
    "\n",
    "In this tutorial, we show how to implement H-Entropy Search procedure [1] in a closed loop in BoTorch.\n",
    "\n",
    "[1]: W. Neiswanger, L. Yu, S. Zhao, C. Meng, S. Ermon. Generalizing Bayesian Optimization with Decision-theoretic Entropies. Appears in Proceedings of the 36th Conference on Neural Information Processing Systems (NeurIPS 2022)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by importing several essential packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tueplots\n",
    "\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from argparse import Namespace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.hentropy_search import (\n",
    "    qHEntropySearch, qLossFunctionTopK, qLossFunctionMinMax\n",
    ")\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.test_functions.synthetic import Ackley\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from matplotlib import cm\n",
    "from tueplots import bundles\n",
    "\n",
    "plt.rcParams.update(bundles.iclr2023())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will explore the 2D Ackley function within the range of [-1, 1]. Our objective is to accomplish two tasks: 1) finding the top-2 values, and 2) determining the min-max of the function. To facilitate the evaluation process, we will define the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_plot(func, cfg, qhes, next_x, data, iteration):\n",
    "    # Quality of the best decision from the current posterior distribution #############\n",
    "    # Initialize A consistently across fantasies\n",
    "    A = torch.rand(\n",
    "        [1, 1, cfg.num_action, cfg.num_dim_action],\n",
    "        requires_grad=True,\n",
    "        device=cfg.device,\n",
    "        dtype=cfg.dtype,\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam([A], lr=cfg.acq_opt_lr)\n",
    "    ba_l, ba_u = cfg.bounds_action\n",
    "\n",
    "    for i in range(cfg.acq_opt_iter):\n",
    "        A_ = A.permute(1, 0, 2, 3)\n",
    "        A_ = torch.sigmoid(A_) * (ba_u - ba_l) + ba_l\n",
    "        posterior = qhes.model.posterior(A_)\n",
    "        fantasized_outcome = qhes.action_sampler(posterior)\n",
    "        # >>> n_fantasy_at_action_pts x n_fantasy_at_design_pts\n",
    "        # ... x batch_size x num_actions x 1\n",
    "\n",
    "        fantasized_outcome = fantasized_outcome.squeeze(dim=-1)\n",
    "        # >>> n_fantasy_at_action_pts x n_fantasy_at_design_pts\n",
    "        # ... x batch_size x num_actions\n",
    "\n",
    "        losses = -qhes.loss_function(A=A_, Y=fantasized_outcome)\n",
    "        # >>> n_fantasy_at_design_pts x batch_size\n",
    "\n",
    "        loss = losses.mean(dim=0).sum()\n",
    "        A.grad = torch.autograd.grad(loss, A)[0]\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(f\"Eval optim round: {i}, Loss: {loss.item():.2f}\")\n",
    "\n",
    "    A = torch.sigmoid(A) * (ba_u - ba_l) + ba_l\n",
    "    eval_metric = qhes.loss_function(A=A, Y=fantasized_outcome)\n",
    "    eval_metric = eval_metric[0, 0].cpu().detach().numpy()\n",
    "    optimal_action = A[0, 0].cpu().detach().numpy()\n",
    "    value = qhes.loss_function(A=A, Y=func(A))[0, 0].cpu().detach().numpy()\n",
    "    data_x = data.x.cpu().detach().numpy()\n",
    "    next_x = next_x.cpu().detach().numpy()\n",
    "\n",
    "    # Plotting #########################################################################\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    bounds_plot = cfg.bounds_design\n",
    "    ax.set(xlabel=\"$x_1$\", ylabel=\"$x_2$\", xlim=bounds_plot, ylim=bounds_plot)\n",
    "    title = \"$\\mathcal{H}_{\\ell, \\mathcal{A}}$-Entropy Search \" + cfg.task\n",
    "    ax.set_title(label=title)\n",
    "\n",
    "    # Plot function in 2D ##############################################################\n",
    "    X_domain, Y_domain = cfg.bounds_design, cfg.bounds_design\n",
    "    n_space = 200\n",
    "    X, Y = np.linspace(*X_domain, n_space), np.linspace(*Y_domain, n_space)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    XY = torch.tensor(np.array([X, Y]))\n",
    "    Z = func(XY.reshape(2, -1).T).reshape(X.shape)\n",
    "    cs = ax.contourf(X, Y, Z, levels=30, cmap=\"bwr\", alpha=0.7)\n",
    "    ax.set_aspect(aspect=\"equal\")\n",
    "    cbar = fig.colorbar(cs)\n",
    "    cbar.ax.set_ylabel(\"$f(x)$\", rotation=270, labelpad=20)\n",
    "\n",
    "    # Plot data, optimal actions, next query ###########################################\n",
    "    ax.scatter(data_x[:, 0], data_x[:, 1], label=\"Data\")\n",
    "    ax.scatter(optimal_action[:, 0], optimal_action[:, 1], label=\"Action\")\n",
    "    ax.scatter(next_x[:, 0], next_x[:, 1], label=\"Next query\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.savefig(f\"{cfg.save_dir}/{iteration}.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "    return eval_metric, optimal_action, value\n",
    "\n",
    "\n",
    "def initialize_model(data, state_dict=None):\n",
    "    model = SingleTaskGP(\n",
    "        train_X=data.x,\n",
    "        train_Y=data.y,\n",
    "        outcome_transform=Standardize(1),\n",
    "    ).to(data.x)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    # load state dict if it is passed\n",
    "    if state_dict is not None:\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    return mll, model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be running the BO loops twice to accomplish two tasks: finding the Min-Max and finding the top-k with k=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exps = [\n",
    "    dict(\n",
    "        task=\"MinMax\",\n",
    "        qLossFunction=qLossFunctionMinMax,\n",
    "        loss_function_hyperparameters=dict(),\n",
    "    ),\n",
    "    dict(\n",
    "        task=\"TopK\",\n",
    "        qLossFunction=qLossFunctionTopK,\n",
    "        loss_function_hyperparameters=dict(\n",
    "            dist_weight=10,\n",
    "            dist_threshold=1,\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "for exp in exps:\n",
    "    # Configure hes trial\n",
    "    cfg = Namespace(\n",
    "        seed=10,\n",
    "        num_iteration=10,\n",
    "        num_initial_points=10,\n",
    "        num_dim_design=2,\n",
    "        num_dim_action=2,\n",
    "        bounds_design=[-1, 1],\n",
    "        bounds_action=[-1, 1],\n",
    "        num_action=2,\n",
    "        n_fantasy_at_design_pts=64,\n",
    "        n_fantasy_at_action_pts=64,\n",
    "        num_restarts=128,\n",
    "        acq_opt_iter=1000,\n",
    "        acq_opt_lr=0.05,\n",
    "        num_candidates=1,\n",
    "        func_is_noisy=False,\n",
    "        func_noise=0.1,\n",
    "        device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "        dtype=torch.double,\n",
    "        loss_function_hyperparameters=exp[\"loss_function_hyperparameters\"],\n",
    "        task=exp[\"task\"],\n",
    "        save_dir=f\"hes_{exp['task']}\",\n",
    "    )\n",
    "\n",
    "    # Make save dir\n",
    "    if not os.path.exists(cfg.save_dir):\n",
    "        os.makedirs(cfg.save_dir)\n",
    "\n",
    "    # Suppress potential optimization warnings for cleaner output\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Set random seeds\n",
    "    np.random.seed(cfg.seed)\n",
    "    random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "    torch.cuda.manual_seed_all(cfg.seed)\n",
    "    # torch.backends.cudnn.deterministic=True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    bd_l, bd_u = cfg.bounds_design\n",
    "    ba_l, ba_u = cfg.bounds_action\n",
    "    f_ = Ackley(dim=cfg.num_dim_design, negate=False)\n",
    "    f_.bounds[0, :].fill_(bd_l)\n",
    "    f_.bounds[1, :].fill_(bd_u)\n",
    "    f_ = f_.to(dtype=cfg.dtype, device=cfg.device)\n",
    "\n",
    "    def func(x):\n",
    "        return f_(x)[..., None]\n",
    "\n",
    "    # Generate initial observations and initialize model\n",
    "    data_x = torch.rand(\n",
    "        [cfg.num_initial_points, cfg.num_dim_design], device=cfg.device, dtype=cfg.dtype\n",
    "    )\n",
    "    data_x = data_x * (bd_u - bd_l) + bd_l\n",
    "    # >>> n x dim\n",
    "\n",
    "    data_y = func(data_x)\n",
    "    # >>> n x 1\n",
    "\n",
    "    if cfg.func_is_noisy:\n",
    "        data_y = data_y + cfg.func_noise * torch.randn_like(data_y)\n",
    "\n",
    "    data = Namespace(x=data_x, y=data_y)\n",
    "\n",
    "    mll, model = initialize_model(data)\n",
    "\n",
    "    # Logging\n",
    "    eval_list = []\n",
    "    eval_data_list = []\n",
    "\n",
    "    # Run BO loop\n",
    "    for j in range(cfg.num_iteration):\n",
    "        # Fit the model\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        # Initialize X and A\n",
    "        X = torch.rand(\n",
    "            [cfg.num_restarts, cfg.num_candidates, cfg.num_dim_design],\n",
    "            requires_grad=True,\n",
    "            device=cfg.device,\n",
    "            dtype=cfg.dtype,\n",
    "        )\n",
    "\n",
    "        A = torch.rand(\n",
    "            [\n",
    "                cfg.num_restarts,\n",
    "                cfg.n_fantasy_at_design_pts,\n",
    "                cfg.num_action,\n",
    "                cfg.num_dim_action,\n",
    "            ],\n",
    "            requires_grad=True,\n",
    "            device=cfg.device,\n",
    "            dtype=cfg.dtype,\n",
    "        )\n",
    "\n",
    "        # define optimizer and objective function\n",
    "        optimizer = torch.optim.Adam([X, A], lr=cfg.acq_opt_lr)\n",
    "        qhes = qHEntropySearch(\n",
    "            model=model,\n",
    "            n_fantasy_at_design_pts=cfg.n_fantasy_at_design_pts,\n",
    "            n_fantasy_at_action_pts=cfg.n_fantasy_at_action_pts,\n",
    "            loss_function_class=exp[\"qLossFunction\"],\n",
    "            loss_function_hyperparameters=cfg.loss_function_hyperparameters,\n",
    "        )\n",
    "        for i in range(cfg.acq_opt_iter):\n",
    "            loss = -qhes(\n",
    "                X=torch.sigmoid(X) * (bd_u - bd_l) + bd_l,\n",
    "                A=torch.sigmoid(A) * (ba_u - ba_l) + ba_l,\n",
    "            ).sum()\n",
    "            \n",
    "            X.grad, A.grad = torch.autograd.grad(loss, [X, A])\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                print(f\"BO round: {j}, Optim round: {i}, Loss: {loss.item():.2f}\")\n",
    "                    \n",
    "        # Get next query and observe the outcome\n",
    "        X = torch.sigmoid(X) * (bd_u - bd_l) + bd_l\n",
    "        A = torch.sigmoid(A) * (ba_u - ba_l) + ba_l\n",
    "        best_id = qhes(X, A).argmax()\n",
    "        next_x = X[best_id].cpu().detach()\n",
    "        action_samples = A[best_id]\n",
    "        next_y = func(next_x).cpu().detach()\n",
    "        print(f\"next_x = {[round(x, 2) for x in next_x.squeeze().numpy().tolist()]}\")\n",
    "        print(f\"next_y = {next_y.squeeze().item():.2f}\")\n",
    "\n",
    "        # Evaluate and plot\n",
    "        eval_metric, optimal_action, value = eval_and_plot(\n",
    "            func=func, cfg=cfg, qhes=qhes, next_x=next_x, data=data, iteration=j\n",
    "        )\n",
    "        eval_list.append(eval_metric)\n",
    "        eval_data_list.append(optimal_action)\n",
    "\n",
    "        # Update training points\n",
    "        data.x = torch.cat([data.x, next_x.to(cfg.device)])\n",
    "        data.y = torch.cat([data.y, next_y.to(cfg.device)])\n",
    "\n",
    "        # Re-initialize model for next iteration, use state_dict to speed up fitting\n",
    "        mll, model = initialize_model(data, model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometric_ampere",
   "language": "python",
   "name": "geometric_ampere"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
