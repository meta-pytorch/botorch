{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Constrained Bayesian Optimization (SCBO)\n",
    "In this tutorial, we show how to implement Scalable Constrained Bayesian Optimization (SCBO) [1] in a closed loop in BoTorch.\n",
    "\n",
    "We optimize the 20ùê∑ Ackley function on the domain [‚àí5,10]20. This implementation uses two simple constraint functions c1 and c2. Our goal is to find values x which maximize Ackley(x) subject to the constraints c1(x) <= 0 and c2(x) <= 0.\n",
    "\n",
    "[1]: David Eriksson and Matthias Poloczek. Scalable constrained Bayesian optimization. In International Conference on Artificial Intelligence and Statistics, pages 730‚Äì738. PMLR, 2021.\n",
    "(https://doi.org/10.48550/arxiv.2002.08526)\n",
    "\n",
    "Since SCBO is essentially a constrained version of Trust Region Bayesina Optimization (TuRBO), this tutorial shares much of the same code as the TuRBO Tutorial (https://botorch.org/tutorials/turbo_1) with small modifications made to implement SCBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.test_functions import Ackley\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from torch.quasirandom import SobolEngine\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.kernels import MaternKernel, ScaleKernel\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "\n",
    "# Constrained Max Posterior Sampling \n",
    "# is a new sampling class, similar to MaxPosteriorSampling, \n",
    "# which implements the constrained version of Thompson Sampling described in [1]\n",
    "from botorch.generation.sampling import ConstrainedMaxPosteriorSampling\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration with 20-dimensional Ackley function and Two Simple Constraint Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the example 20D Ackley function \n",
    "fun = Ackley(dim=20, negate=True).to(dtype=dtype, device=device)\n",
    "fun.bounds[0, :].fill_(-5)\n",
    "fun.bounds[1, :].fill_(10)\n",
    "dim = fun.dim\n",
    "lb, ub = fun.bounds\n",
    "\n",
    "batch_size = 4\n",
    "n_init = 2 * dim\n",
    "max_cholesky_size = float(\"inf\")  # Always use Cholesky\n",
    "\n",
    "# When evaluating the function, we must first unnormalize the inputs since \n",
    "# we will use normalized inputs x in the main optimizaiton loop\n",
    "def eval_objective(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return fun(unnormalize(x, fun.bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining two simple constraint functions\n",
    "\n",
    "#### We'll use two constraints functions: c1 and c2 \n",
    "We want to find solutions which maximize the above Ackley objective subject tot he constraint that \n",
    "c1(x) <= 0 and c2(x) <= 0 \n",
    "Note that SCBO expects all constraints to be of the for c(x) <= 0, so any other desired constraints must be modified to fit this form. \n",
    "\n",
    "Note also that while the below constraints are very simple functions, the point of this tutorial is to show how to use SCBO, and this same implementation could be applied in the same way if c1, c2 were actually complex black-box functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c1(x): # Equivalent to enforcing that x[0] >= 0\n",
    "    return -x[0] \n",
    "\n",
    "def c2(x): # Equivalent to enforcing that x[1] >= 0\n",
    "    return -x[1] \n",
    "\n",
    "# We assume c1, c2 have same bounds as the Ackley function above \n",
    "def eval_c1(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return c1(unnormalize(x, fun.bounds))\n",
    "\n",
    "def eval_c2(x):\n",
    "    \"\"\"This is a helper function we use to unnormalize and evalaute a point\"\"\"\n",
    "    return c2(unnormalize(x, fun.bounds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define TuRBO Class\n",
    "\n",
    "Just as in the TuRBO Tutorial (https://botorch.org/tutorials/turbo_1), we'll define a class to hold the turst region state and a method update_state() to update the lengths of the trust region as discussed in the original TuRBO paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TurboState(dim=20, batch_size=4, length=0.8, length_min=0.0078125, length_max=1.6, failure_counter=0, failure_tolerance=5, success_counter=0, success_tolerance=10, best_value=-inf, restart_triggered=False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TurboState:\n",
    "    dim: int\n",
    "    batch_size: int\n",
    "    length: float = 0.8\n",
    "    length_min: float = 0.5 ** 7\n",
    "    length_max: float = 1.6\n",
    "    failure_counter: int = 0\n",
    "    failure_tolerance: int = float(\"nan\")  # Note: Post-initialized\n",
    "    success_counter: int = 0\n",
    "    success_tolerance: int = 10  # Note: The original paper uses 3\n",
    "    best_value: float = -float(\"inf\")\n",
    "    restart_triggered: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.failure_tolerance = math.ceil(\n",
    "            max([4.0 / self.batch_size, float(self.dim) / self.batch_size])\n",
    "        )\n",
    "\n",
    "\n",
    "def update_state(state, Y_next):\n",
    "    if max(Y_next) > state.best_value + 1e-3 * math.fabs(state.best_value):\n",
    "        state.success_counter += 1\n",
    "        state.failure_counter = 0\n",
    "    else:\n",
    "        state.success_counter = 0\n",
    "        state.failure_counter += 1\n",
    "\n",
    "    if state.success_counter == state.success_tolerance:  # Expand trust region\n",
    "        state.length = min(2.0 * state.length, state.length_max)\n",
    "        state.success_counter = 0\n",
    "    elif state.failure_counter == state.failure_tolerance:  # Shrink trust region\n",
    "        state.length /= 2.0\n",
    "        state.failure_counter = 0\n",
    "\n",
    "    state.best_value = max(state.best_value, max(Y_next).item())\n",
    "    if state.length < state.length_min:\n",
    "        state.restart_triggered = True\n",
    "    return state\n",
    "\n",
    "# Define example state \n",
    "state = TurboState(dim=dim, batch_size=batch_size)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Initial Points\n",
    "\n",
    "Here we define a simple method to generate a set of random initial datapoints that we will use to kick-off optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_points(dim, n_pts, seed=0):\n",
    "    sobol = SobolEngine(dimension=dim, scramble=True, seed=seed)\n",
    "    X_init = sobol.draw(n=n_pts).to(dtype=dtype, device=device)\n",
    "    return X_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate batch modifications for SCBO \n",
    "\n",
    "Just as in the TuRBO Tutorial (https://botorch.org/tutorials/turbo_1), we'll define a method generate_batch to generate a new batch of candidate points within the TuRBO trust region using thompson sampling. \n",
    "\n",
    "The key difference here from TuRBO is that, instead of using MaxPosteriorSampling to simply grab the candidates within the trust region with the maximum posterior values, we use ConstrainedMaxPosteriorSampling to instead grab the candidates within the trust region with the maximum posterior values subject to the constrain that the posteriors for the constraint models for c1(x) and c2(x) must be less than or equal to 0 for each candidate. In otherwords, we use additional GPs ('constraiant models') to model each black-box constraint (c1 and c2), and throw out all candidates for which the posterior prediction of these constraint models is greater than 0 (throw out all predicted constraint violators). According to [1], in the special case when all of the candidaates arae predicted to be constraint violators, we select the candidate with the minimum predicted violation. (See botorch.generation.sampling.ConstrainedMaxPosteriorSampling for implementation details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(\n",
    "    state,\n",
    "    model,  # GP model\n",
    "    X,  # Evaluated points on the domain [0, 1]^d\n",
    "    Y,  # Function values\n",
    "    batch_size,\n",
    "    n_candidates=None,  # Number of candidates for Thompson sampling\n",
    "    constraint_model=None\n",
    "):\n",
    "    assert X.min() >= 0.0 and X.max() <= 1.0 and torch.all(torch.isfinite(Y))\n",
    "    if n_candidates is None:\n",
    "        n_candidates = min(5000, max(2000, 200 * X.shape[-1]))\n",
    "\n",
    "    # Scale the TR to be proportional to the lengthscales\n",
    "    x_center = X[Y.argmax(), :].clone()\n",
    "    weights = model.covar_module.base_kernel.lengthscale.squeeze().detach()\n",
    "    weights = weights / weights.mean()\n",
    "    weights = weights / torch.prod(weights.pow(1.0 / len(weights)))\n",
    "    tr_lb = torch.clamp(x_center - weights * state.length / 2.0, 0.0, 1.0)\n",
    "    tr_ub = torch.clamp(x_center + weights * state.length / 2.0, 0.0, 1.0)\n",
    "\n",
    "    # Thompson Sampling w/ Constraints (SCBO) \n",
    "    dim = X.shape[-1]\n",
    "    sobol = SobolEngine(dim, scramble=True)\n",
    "    pert = sobol.draw(n_candidates).to(dtype=dtype, device=device)\n",
    "    pert = tr_lb + (tr_ub - tr_lb) * pert\n",
    "\n",
    "    # Create a perturbation mask\n",
    "    prob_perturb = min(20.0 / dim, 1.0)\n",
    "    mask = (\n",
    "        torch.rand(n_candidates, dim, dtype=dtype, device=device)\n",
    "        <= prob_perturb\n",
    "    )\n",
    "    ind = torch.where(mask.sum(dim=1) == 0)[0]\n",
    "    mask[ind, torch.randint(0, dim - 1, size=(len(ind),), device=device)] = 1\n",
    "\n",
    "    # Create candidate points from the perturbations and the mask        \n",
    "    X_cand = x_center.expand(n_candidates, dim).clone()\n",
    "    X_cand[mask] = pert[mask]\n",
    "\n",
    "    # Sample on the candidate points using Constrained Max Posterior Sampling\n",
    "    constrained_thompson_sampling = ConstrainedMaxPosteriorSampling(model=model, constraint_model=constraint_model, replacement=False)\n",
    "    with torch.no_grad():  \n",
    "        X_next = constrained_thompson_sampling(X_cand, num_samples=batch_size)\n",
    "\n",
    "\n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.6854, device='cuda:0'),\n",
       " tensor(4.8509, device='cuda:0'),\n",
       " tensor(-9.8032, device='cuda:0'),\n",
       " tensor(4.7317, device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get initial data \n",
    "# Must get initial values for both objective and constraints \n",
    "X_turbo = get_initial_points(dim, n_init)\n",
    "Y_turbo = torch.tensor(\n",
    "    [eval_objective(x) for x in X_turbo], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "C1_turbo = torch.tensor(\n",
    "    [eval_c1(x) for x in X_turbo], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "C2_turbo = torch.tensor(\n",
    "    [eval_c2(x) for x in X_turbo], dtype=dtype, device=device\n",
    ").unsqueeze(-1)\n",
    "\n",
    "C1_turbo.min(), C1_turbo.max(), C2_turbo.min(), C2_turbo.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the suggested candidates met the constraints...\n",
      "44) Best value: -inf, TR length: 8.00e-01\n",
      "None of the suggested candidates met the constraints...\n",
      "48) Best value: -inf, TR length: 8.00e-01\n",
      "52) Best value: -1.24e+01, TR length: 8.00e-01\n",
      "56) Best value: -1.22e+01, TR length: 8.00e-01\n",
      "60) Best value: -1.13e+01, TR length: 8.00e-01\n",
      "64) Best value: -1.03e+01, TR length: 8.00e-01\n",
      "68) Best value: -1.03e+01, TR length: 8.00e-01\n",
      "72) Best value: -1.03e+01, TR length: 8.00e-01\n",
      "76) Best value: -1.03e+01, TR length: 8.00e-01\n",
      "80) Best value: -1.03e+01, TR length: 8.00e-01\n",
      "84) Best value: -1.03e+01, TR length: 4.00e-01\n",
      "88) Best value: -9.54e+00, TR length: 4.00e-01\n",
      "92) Best value: -9.34e+00, TR length: 4.00e-01\n",
      "96) Best value: -8.60e+00, TR length: 4.00e-01\n",
      "100) Best value: -8.03e+00, TR length: 4.00e-01\n",
      "104) Best value: -7.93e+00, TR length: 4.00e-01\n",
      "108) Best value: -7.93e+00, TR length: 4.00e-01\n",
      "112) Best value: -7.93e+00, TR length: 4.00e-01\n",
      "116) Best value: -7.93e+00, TR length: 4.00e-01\n",
      "120) Best value: -7.93e+00, TR length: 4.00e-01\n",
      "124) Best value: -7.93e+00, TR length: 2.00e-01\n",
      "128) Best value: -7.89e+00, TR length: 2.00e-01\n",
      "132) Best value: -7.11e+00, TR length: 2.00e-01\n",
      "136) Best value: -7.11e+00, TR length: 2.00e-01\n",
      "140) Best value: -7.11e+00, TR length: 2.00e-01\n",
      "144) Best value: -7.11e+00, TR length: 2.00e-01\n",
      "148) Best value: -7.11e+00, TR length: 2.00e-01\n",
      "152) Best value: -7.11e+00, TR length: 1.00e-01\n",
      "156) Best value: -7.11e+00, TR length: 1.00e-01\n",
      "160) Best value: -6.95e+00, TR length: 1.00e-01\n",
      "164) Best value: -6.95e+00, TR length: 1.00e-01\n",
      "168) Best value: -6.94e+00, TR length: 1.00e-01\n",
      "172) Best value: -6.82e+00, TR length: 1.00e-01\n",
      "176) Best value: -6.80e+00, TR length: 1.00e-01\n",
      "180) Best value: -6.72e+00, TR length: 1.00e-01\n",
      "184) Best value: -6.72e+00, TR length: 1.00e-01\n",
      "188) Best value: -6.71e+00, TR length: 1.00e-01\n",
      "192) Best value: -6.51e+00, TR length: 1.00e-01\n",
      "196) Best value: -6.51e+00, TR length: 1.00e-01\n",
      "200) Best value: -6.51e+00, TR length: 1.00e-01\n",
      "204) Best value: -6.22e+00, TR length: 1.00e-01\n",
      "208) Best value: -6.22e+00, TR length: 1.00e-01\n",
      "212) Best value: -6.22e+00, TR length: 1.00e-01\n",
      "216) Best value: -6.22e+00, TR length: 1.00e-01\n",
      "220) Best value: -6.22e+00, TR length: 1.00e-01\n",
      "224) Best value: -6.22e+00, TR length: 5.00e-02\n",
      "228) Best value: -6.22e+00, TR length: 5.00e-02\n",
      "232) Best value: -6.22e+00, TR length: 5.00e-02\n",
      "236) Best value: -6.22e+00, TR length: 5.00e-02\n",
      "240) Best value: -6.18e+00, TR length: 5.00e-02\n",
      "244) Best value: -6.18e+00, TR length: 5.00e-02\n",
      "248) Best value: -6.18e+00, TR length: 5.00e-02\n",
      "252) Best value: -6.18e+00, TR length: 5.00e-02\n",
      "256) Best value: -6.18e+00, TR length: 5.00e-02\n",
      "260) Best value: -6.18e+00, TR length: 2.50e-02\n",
      "264) Best value: -5.98e+00, TR length: 2.50e-02\n",
      "268) Best value: -5.96e+00, TR length: 2.50e-02\n",
      "272) Best value: -5.96e+00, TR length: 2.50e-02\n",
      "276) Best value: -5.96e+00, TR length: 2.50e-02\n",
      "280) Best value: -5.95e+00, TR length: 2.50e-02\n",
      "284) Best value: -5.95e+00, TR length: 2.50e-02\n",
      "288) Best value: -5.95e+00, TR length: 2.50e-02\n",
      "292) Best value: -5.95e+00, TR length: 2.50e-02\n",
      "296) Best value: -5.95e+00, TR length: 2.50e-02\n",
      "300) Best value: -5.95e+00, TR length: 1.25e-02\n",
      "304) Best value: -5.87e+00, TR length: 1.25e-02\n",
      "308) Best value: -5.87e+00, TR length: 1.25e-02\n",
      "312) Best value: -5.65e+00, TR length: 1.25e-02\n",
      "316) Best value: -5.65e+00, TR length: 1.25e-02\n",
      "320) Best value: -5.65e+00, TR length: 1.25e-02\n",
      "324) Best value: -5.65e+00, TR length: 1.25e-02\n",
      "328) Best value: -5.65e+00, TR length: 1.25e-02\n",
      "332) Best value: -5.65e+00, TR length: 6.25e-03\n"
     ]
    }
   ],
   "source": [
    "# Initialize TuRBO state \n",
    "state = TurboState(dim, batch_size=batch_size)\n",
    "N_CANDIDATES = min(5000, max(2000, 200 * dim)) \n",
    "\n",
    " \n",
    "while not state.restart_triggered:  # Run until TuRBO converges\n",
    "    # Fit a GP model for objective \n",
    "    train_Y = (Y_turbo - Y_turbo.mean()) / Y_turbo.std()\n",
    "    likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0))\n",
    "    )\n",
    "    model = SingleTaskGP(X_turbo, train_Y, covar_module=covar_module, likelihood=likelihood)\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "\n",
    "    # Fit a GP model for C1 \n",
    "    train_C1 = (C1_turbo - C1_turbo.mean()) / C1_turbo.std()\n",
    "    c1_likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    c1_covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0))\n",
    "    )\n",
    "    c1_model = SingleTaskGP(X_turbo, train_C1, covar_module=c1_covar_module, likelihood=c1_likelihood)\n",
    "    c1_mll = ExactMarginalLogLikelihood(c1_model.likelihood, c1_model)\n",
    "\n",
    "    # Fit a GP model for C2 \n",
    "    train_C2 = (C2_turbo - C2_turbo.mean()) / C2_turbo.std()\n",
    "    c2_likelihood = GaussianLikelihood(noise_constraint=Interval(1e-8, 1e-3))\n",
    "    c2_covar_module = ScaleKernel(  # Use the same lengthscale prior as in the TuRBO paper\n",
    "        MaternKernel(nu=2.5, ard_num_dims=dim, lengthscale_constraint=Interval(0.005, 4.0))\n",
    "    )\n",
    "    c2_model = SingleTaskGP(X_turbo, train_C2, covar_module=c2_covar_module, likelihood=c2_likelihood)\n",
    "    c2_mll = ExactMarginalLogLikelihood(c2_model.likelihood, c2_model)\n",
    "\n",
    "    # Do the fitting and acquisition function optimization inside the Cholesky context\n",
    "    with gpytorch.settings.max_cholesky_size(max_cholesky_size):\n",
    "        # Fit the models\n",
    "        fit_gpytorch_model(mll)\n",
    "        fit_gpytorch_model(c1_mll)\n",
    "        fit_gpytorch_model(c2_mll)\n",
    "    \n",
    "        # Generate a batch of candidates\n",
    "        X_next = generate_batch(\n",
    "            state=state,\n",
    "            model=model,\n",
    "            X=X_turbo,\n",
    "            Y=train_Y,\n",
    "            batch_size=batch_size,\n",
    "            n_candidates=N_CANDIDATES,\n",
    "            constraint_model=ModelListGP(c1_model, c2_model)    \n",
    "        )\n",
    "\n",
    "    # Evaluate both the objective and constraints for the selected candidaates \n",
    "    Y_next = torch.tensor(\n",
    "        [eval_objective(x) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1) \n",
    "\n",
    "    C1_next = torch.tensor(\n",
    "        [eval_c1(x) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1) \n",
    "\n",
    "    C2_next = torch.tensor( \n",
    "        [eval_c2(x) for x in X_next], dtype=dtype, device=device\n",
    "    ).unsqueeze(-1) \n",
    "\n",
    "    # Grab the truly valid samples (Valid_Y_next)\n",
    "    #   (valid samples must have BOTH c1 <= 0 and c2 <= 0 )\n",
    "    constraint_vals = torch.cat([C1_next, C2_next], dim=-1) \n",
    "    bool_tensor = constraint_vals <= 0 \n",
    "    bool_tensor = torch.all(bool_tensor, dim=-1).unsqueeze(-1) \n",
    "    Valid_Y_next = Y_next[bool_tensor] \n",
    "\n",
    "    # Update state only with VALID samples only \n",
    "    #   ie we only want to count a success and update \n",
    "    #   our current best point found when we have \n",
    "    #   found a new point that both improves upon our current best,\n",
    "    #   and meets all constraints\n",
    "    # Only update the state if there are new valid points to update with\n",
    "    if Valid_Y_next.numel() > 0: \n",
    "        state = update_state(state=state, Y_next=Valid_Y_next)\n",
    "    else:\n",
    "        print(\"None of the suggested candidates met the constraints...\")\n",
    "\n",
    "    # Append data\n",
    "    #   Notice we append all data, even points that violate\n",
    "    #   the constriants, this is so our constraint models\n",
    "    #   can learn more about the constranit functions and \n",
    "    #   gain confidence about where violation occurs\n",
    "    X_turbo = torch.cat((X_turbo, X_next), dim=0)\n",
    "    Y_turbo = torch.cat((Y_turbo, Y_next), dim=0)\n",
    "    C1_turbo = torch.cat((C1_turbo, C1_next), dim=0)\n",
    "    C2_turbo = torch.cat((C2_turbo, C2_next), dim=0)\n",
    "\n",
    "    # Print current status \n",
    "    #   Note: Since only valid y's that met the constraints were used to update the \n",
    "    #   trust region state, state.best_value is always the best objective value\n",
    "    #   found so far which meets the constraints \n",
    "    print(\n",
    "        f\"{len(X_turbo)}) Best value: {state.best_value:.2e}, TR length: {state.length:.2e}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With constraints, the best value we found is: -5.6503\n"
     ]
    }
   ],
   "source": [
    "#  Valid samples must have BOTH c1 <= 0 and c2 <= 0  \n",
    "constraint_vals = torch.cat([C1_turbo, C2_turbo], dim=-1) \n",
    "bool_tensor = constraint_vals <= 0 \n",
    "bool_tensor = torch.all(bool_tensor, dim=-1).unsqueeze(-1) \n",
    "Valid_Y = Y_turbo[bool_tensor] \n",
    "\n",
    "print(f\"With constraints, the best value we found is: {Valid_Y.max().item():.4f}\") "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9beb4c3e6521665a47c2b1e65f245d1b2309f4194f15ed6955f5e52622a9d29e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
