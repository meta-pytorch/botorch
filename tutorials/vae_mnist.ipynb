{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE MNIST Example: BO in a Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem setup\n",
    "\n",
    "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
    "\n",
    "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
    "\\longrightarrow \\text{score}.$$\n",
    "\n",
    "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next instantiate the CNN for digit recognition and load a pre-trained model.\n",
    "\n",
    "Here, you may have to change `PRETRAINED_LOCATION` to the location of the `pretrained_models` folder on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_LOCATION = \"./pretrained_models\"\n",
    "\n",
    "cnn_model = Net().to(device)\n",
    "cnn_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_cnn.pt\"), map_location=device)\n",
    "cnn_model.load_state_dict(cnn_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "vae_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_vae.pt\"), map_location=device)\n",
    "vae_model.load_state_dict(vae_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y):\n",
    "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
    "    centered at the digit '3'.\n",
    "    \"\"\"\n",
    "    return torch.exp(-2 * (y - 3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_recognition(x):\n",
    "    \"\"\"The input x is an image and an expected score based on the CNN classifier and\n",
    "    the scoring function is returned.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.exp(cnn_model(x))  # b x 10\n",
    "        scores = score(torch.arange(10, device=device, dtype=dtype)).expand(probs.shape)\n",
    "    return (probs * scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization and initial random batch\n",
    "\n",
    "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "bounds = torch.tensor([[-6.0] * 20, [6.0] * 20], device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def initialize_model(n=5):\n",
    "    # generate training data  \n",
    "    train_x = (bounds[1] - bounds[0]) * torch.rand(n, 20, device=device, dtype=dtype) + bounds[0]\n",
    "    train_obj = score_image_recognition(decode(train_x))\n",
    "    best_observed_value = train_obj.max().item()\n",
    "    \n",
    "    # define models for objective and constraint\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "    model = model.to(train_x)\n",
    "    \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll = mll.to(train_x)\n",
    "    \n",
    "    return train_x, train_obj, mll, model, best_observed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim import joint_optimize\n",
    "\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation\"\"\"\n",
    "    \n",
    "    # optimize\n",
    "    candidates = joint_optimize(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=10,\n",
    "        raw_samples=100,\n",
    "        options={\"simple_init\": True, \"maxiter\": 200},\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x = candidates.detach()\n",
    "    new_obj = score_image_recognition(decode(new_x))\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Bayesian Optimization loop with qEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2000` samples. We also initialize the model with 5 randomly drawn points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.acquisition.sampler import SobolQMCNormalSampler\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N_BATCH = 75\n",
    "MC_SAMPLES = 2000\n",
    "best_observed = []\n",
    "\n",
    "# call helper function to initialize model\n",
    "train_x, train_obj, mll, model, best_value = initialize_model(n=5)\n",
    "best_observed.append(best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running BO ..........................................................................."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"\\nRunning BO \", end='')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(N_BATCH):    \n",
    "\n",
    "    # fit the model\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # define the qNEI acquisition module using a QMC sampler\n",
    "    qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=seed)\n",
    "    qEI = qExpectedImprovement(model=model, sampler=qmc_sampler, best_f=best_value)\n",
    "\n",
    "    # optimize and get new observation\n",
    "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
    "\n",
    "    # update training points\n",
    "    train_x = torch.cat((train_x, new_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    # update progress\n",
    "    best_value = score_image_recognition(decode(train_x)).max().item()\n",
    "    best_observed.append(best_value)\n",
    "\n",
    "    # reinitialize the model so it is ready for fitting on next iteration\n",
    "    model.set_train_data(train_x, train_obj, strict=False)\n",
    "    \n",
    "    print(\".\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
    "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
    "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
    "\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    b = torch.argmax(score_image_recognition(decode(train_x[:inds[i],:])), dim=0)\n",
    "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
    "    ax.imshow(img, alpha=0.8, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
