{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE MNIST example: BO in a latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets # transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup\n",
    "\n",
    "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
    "\n",
    "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
    "\\longrightarrow \\text{score}.$$\n",
    "\n",
    "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next instantiate the CNN for digit recognition and load a pre-trained model.\n",
    "\n",
    "Here, you may have to change `PRETRAINED_LOCATION` to the location of the `pretrained_models` folder on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_LOCATION = \"./pretrained_models\"\n",
    "\n",
    "cnn_model = Net().to(device)\n",
    "cnn_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_cnn.pt\"), map_location=device)\n",
    "cnn_model.load_state_dict(cnn_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "vae_model = VAE().to(device)\n",
    "vae_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_vae.pt\"), map_location=device)\n",
    "vae_model.load_state_dict(vae_state_dict);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y):\n",
    "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
    "    centered at the digit '3'.\n",
    "    \"\"\"\n",
    "    return torch.exp(-2 * (y - 3)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_image_recognition(x):\n",
    "    \"\"\"The input x is an image and an expected score based on the CNN classifier and\n",
    "    the scoring function is returned.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        probs = torch.exp(cnn_model(x))  # b x 10\n",
    "        scores = score(torch.arange(10, device=device, dtype=dtype)).expand(probs.shape)\n",
    "    return (probs * scores).sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(train_x):\n",
    "    with torch.no_grad():\n",
    "        decoded = vae_model.decode(train_x)\n",
    "    return decoded.view(train_x.shape[0], 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model initialization and initial random batch\n",
    "\n",
    "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "\n",
    "\n",
    "bounds = torch.tensor([[-6.0] * 20, [6.0] * 20], device=device, dtype=dtype)\n",
    "\n",
    "\n",
    "def initialize_model(n=5):\n",
    "    # generate training data  \n",
    "    train_x = (bounds[1] - bounds[0]) * torch.rand(n, 20, device=device, dtype=dtype) + bounds[0]\n",
    "    train_obj = score_image_recognition(decode(train_x))\n",
    "    best_observed_value = train_obj.max().item()\n",
    "    \n",
    "    # define models for objective and constraint\n",
    "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
    "    model = model.to(train_x)\n",
    "    \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    mll = mll.to(train_x)\n",
    "    \n",
    "    return train_x, train_obj, mll, model, best_observed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a helper function that performs the essential BO step\n",
    "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim import joint_optimize\n",
    "\n",
    "\n",
    "BATCH_SIZE = 3\n",
    "\n",
    "\n",
    "def optimize_acqf_and_get_observation(acq_func):\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation\"\"\"\n",
    "    \n",
    "    # optimize\n",
    "    candidates = joint_optimize(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=10,\n",
    "        raw_samples=200,\n",
    "    )\n",
    "\n",
    "    # observe new values \n",
    "    new_x = candidates.detach()\n",
    "    new_obj = score_image_recognition(decode(new_x))\n",
    "    return new_x, new_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Bayesian Optimization loop with qEI\n",
    "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2000` samples. We also initialize the model with 5 randomly drawn points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
    "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "N_BATCH = 50\n",
    "MC_SAMPLES = 2000\n",
    "best_observed = []\n",
    "\n",
    "# call helper function to initialize model\n",
    "train_x, train_obj, mll, model, best_value = initialize_model(n=5)\n",
    "best_observed.append(best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running BO .................................................."
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(f\"\\nRunning BO \", end='')\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
    "for iteration in range(N_BATCH):    \n",
    "\n",
    "    # fit the model\n",
    "    fit_gpytorch_model(mll)\n",
    "\n",
    "    # define the qNEI acquisition module using a QMC sampler\n",
    "    qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=seed)\n",
    "    qEI = qExpectedImprovement(model=model, sampler=qmc_sampler, best_f=best_value)\n",
    "\n",
    "    # optimize and get new observation\n",
    "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
    "\n",
    "    # update training points\n",
    "    train_x = torch.cat((train_x, new_x))\n",
    "    train_obj = torch.cat((train_obj, new_obj))\n",
    "\n",
    "    # update progress\n",
    "    best_value = score_image_recognition(decode(train_x)).max().item()\n",
    "    best_observed.append(best_value)\n",
    "\n",
    "    # reinitialize the model so it is ready for fitting on next iteration\n",
    "    model.set_train_data(train_x, train_obj, strict=False)\n",
    "    \n",
    "    print(\".\", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x1008 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
    "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
    "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
    "\n",
    "for i, ax in enumerate(ax.flat):\n",
    "    b = torch.argmax(score_image_recognition(decode(train_x[:inds[i],:])), dim=0)\n",
    "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
    "    ax.imshow(img, alpha=0.8, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
